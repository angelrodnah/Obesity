{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Riesgo de Obesidad y problemas CardioVasculares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga funciones propias reutilizables  de librerías\n",
    "import os as so\n",
    "import sys\n",
    "utils_path = so.path.join(so.getcwd(), '..', 'utils')\n",
    "sys.path.append(utils_path)\n",
    "class PATH():\n",
    "    RAW = './data/raw//'\n",
    "    PROCESSED= './data/processed//'\n",
    "    MODELS = './models//'\n",
    "    REPORTS = './reports//'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(PATH.RAW + 'train.csv')\n",
    "df_test = pd.read_csv(PATH.RAW +'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_columnas = {\n",
    "    'FAVC': 'Consumo frecuente de alimentos altos en calorías',\n",
    "    'FCVC': 'Frecuencia de consumo de vegetales',\n",
    "    'NCP': 'Número de comidas principales',\n",
    "    'CAEC': 'Consumo de alimentos entre comidas',\n",
    "    'CH20': 'Consumo de agua diario',\n",
    "    'CALC': 'Consumo de alcohol',\n",
    "    'SCC': 'Monitoreo del consumo de calorías',\n",
    "    'FAF': 'Frecuencia de actividad física',\n",
    "    'TUE': 'Tiempo utilizando dispositivos tecnológicos',\n",
    "    'MTRANS': 'Transporte utilizado',\n",
    "    'Age':'Edad',\n",
    "    'Height': 'Altura',\n",
    "    'Weight':'Peso',\n",
    "    'SMOKE':'Fumador',\n",
    "    'Gender':'Género',\n",
    "    'IMC':'Indice de Masa Corporal',\n",
    "    'HA': 'Hábitos alimienticios',\n",
    "    'UT': 'Uso de Tecnología',\n",
    "    'family_history_with_overweight':'Historial familiar con sobrepeso'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"NObeyesdad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_target={'Insufficient_Weight': 0,\n",
    " 'Normal_Weight': 1,\n",
    " 'Overweight_Level_I': 2,\n",
    " 'Overweight_Level_II': 3,\n",
    "  'Obesity_Type_I': 4,\n",
    " 'Obesity_Type_II': 5,\n",
    " 'Obesity_Type_III': 6,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictrev_target={v:k for k,v in  dict_target.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = df.columns[df.dtypes==\"object\"].tolist()\n",
    "numericas = df.columns[df.dtypes!=\"object\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].value_counts() / len(df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay columnas que tienen orden y que deben convertirse a variables discretas. Las mapeo también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_CAEC={\n",
    "    'no': 0,\n",
    "    'Sometimes': 1,\n",
    "    'Frequently': 2,\n",
    "    'Always': 3}\n",
    "dict_CALC={\n",
    "    'no': 0,\n",
    "    'Sometimes': 1,\n",
    "    'Frequently': 2,\n",
    "    'Always': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering(df):\n",
    "    df.set_index('id', inplace=True)\n",
    "    df['IMC'] = df['Weight'] / (df['Height'] ** 2)\n",
    "    df['HA'] = df['FCVC'] * df['NCP']\n",
    "    df['UT'] = df['TUE'] / df['Age']\n",
    "    df['CALC']=df['CALC'].map(dict_CALC)\n",
    "    df['CAEC']=df['CAEC'].map(dict_CAEC)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Feature_Engineering(df_train)\n",
    "df_test = Feature_Engineering(df_test)\n",
    "df = Feature_Engineering(df)\n",
    "numericas = df.columns[df.dtypes!=\"object\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_columnas = {\n",
    "    'FAVC': 'Consumo frecuente de alimentos altos en calorías',\n",
    "    'FCVC': 'Frecuencia de consumo de vegetales',\n",
    "    'NCP': 'Número de comidas principales',\n",
    "    'CAEC': 'Consumo de alimentos entre comidas',\n",
    "    'CH20': 'Consumo de agua diario',\n",
    "    'CALC': 'Consumo de alcohol',\n",
    "    'SCC': 'Monitoreo del consumo de calorías',\n",
    "    'FAF': 'Frecuencia de actividad física',\n",
    "    'TUE': 'Tiempo utilizando dispositivos tecnológicos',\n",
    "    'MTRANS': 'Transporte utilizado',\n",
    "    'Age':'Edad',\n",
    "    'Height': 'Altura',\n",
    "    'Weight':'Peso',\n",
    "    'SMOKE':'Fumador',\n",
    "    'Gender':'Género',\n",
    "    'IMC':'Indice de Masa Corporal',\n",
    "    'HA': 'Hábitos alimienticios',\n",
    "    'UT': 'Uso de Tecnología',\n",
    "    'family_history_with_overweight':'Historial familiar con sobrepeso'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones (Encoding)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = df.columns[df.dtypes==\"object\"].tolist()\n",
    "numericas = df.columns[df.dtypes!=\"object\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_cols(df, max_cardi=20, max_miss=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_transform(X):\n",
    "    return np.sqrt(X)\n",
    "\n",
    "def log_transform(X):\n",
    "    return np.log1p(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target no necesita transformación\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo df, df_train y df_test antes de transformarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df as pickle\n",
    "df.to_pickle(PATH.PROCESSED + 'df.pkl')\n",
    "df_train.to_pickle(PATH.PROCESSED + 'df_train.pkl')\n",
    "df_test.to_pickle(PATH.PROCESSED + 'df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df as pickle\n",
    "df = pd.read_pickle(PATH.PROCESSED + 'df.pkl')\n",
    "df_train = pd.read_pickle(PATH.PROCESSED + 'df_train.pkl')\n",
    "df_test = pd.read_pickle(PATH.PROCESSED + 'df_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline para transformar las columnas. Para ver la significancia, lo tengo que pasar a numéricas y ya veré si posteriormente elimino columnas antes de quitar outliners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_class = classify_distributions(df, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(df, target_col, threshold=0.05):\n",
    "    if target_col not in df.columns:\n",
    "        X = df.copy()\n",
    "    else:\n",
    "        X = df.drop(columns=[target_col])\n",
    "\n",
    "    dist_class = classify_distributions(X, threshold)\n",
    "    \n",
    "    categorical_columns = list(X.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    if categorical_columns:\n",
    "        categorical_pipeline = Pipeline([\n",
    "            ('onehot', OneHotEncoder(drop='first'))\n",
    "        ])\n",
    "    else:\n",
    "        categorical_pipeline = None\n",
    "    \n",
    "    numeric_columns_to_transform = [col for col, (dist,model) in dist_class.items()]\n",
    "    untouched_columns = list(set(X.columns) - set(categorical_columns) - set(numeric_columns_to_transform))\n",
    "\n",
    "    numeric_transformers = [\n",
    "        ('sqrt_transform', FunctionTransformer(sqrt_transform), [col for col, (dist, _) in dist_class.items() if dist == 'positive_increasing']),\n",
    "        ('log_transform', FunctionTransformer(log_transform), [col for col, (dist, _) in dist_class.items() if dist == 'positive_decreasing']),\n",
    "        ('yeojohnson_transform', PowerTransformer(method='yeo-johnson'), [col for col, (dist, _) in dist_class.items() if dist not in ['positive_increasing', 'positive_decreasing']]),\n",
    "        ('untouched', 'passthrough', untouched_columns)\n",
    "    ]\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('transformation', ColumnTransformer(\n",
    "            transformers=numeric_transformers,\n",
    "            remainder='passthrough')\n",
    "        ),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    if categorical_pipeline:\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('categorical', categorical_pipeline, categorical_columns),\n",
    "            ('numeric', numeric_pipeline, numeric_columns_to_transform)\n",
    "        ])\n",
    "    else:\n",
    "        preprocessor = numeric_pipeline\n",
    "\n",
    "    final_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "\n",
    "    required_columns = set([col for col, _ in dist_class.items()])\n",
    "    if not required_columns.issubset(X.columns):\n",
    "        missing_columns = required_columns - set(X.columns)\n",
    "        raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "    X_trans = final_pipeline.fit_transform(X)\n",
    "    transformed_columns = []\n",
    "\n",
    "    # Obtener columnas transformadas de OHE\n",
    "    if categorical_columns:\n",
    "        ohe_columns = final_pipeline.named_steps['preprocessor'].named_transformers_['categorical'].named_steps['onehot'].get_feature_names_out()\n",
    "        transformed_columns.extend(ohe_columns)\n",
    "\n",
    "    # Obtener columnas transformadas de variables numéricas\n",
    "    numeric_transformer = final_pipeline.named_steps['preprocessor'].named_transformers_['numeric'].named_steps['transformation']\n",
    "    numeric_columns_transformed = [col for name, transformer, col in numeric_transformer.transformers_ if transformer != 'drop']\n",
    "    transformed_columns.extend(numeric_columns_transformed)\n",
    "\n",
    "    # Obtener columnas restantes\n",
    "    remaining_columns = list(X.columns.difference(categorical_columns + numeric_columns_to_transform))\n",
    "    transformed_columns.extend(remaining_columns)\n",
    "    transformed_columns=list(pd.DataFrame(transformed_columns)[0].explode().dropna())\n",
    "    \n",
    "    return pd.DataFrame(X_trans, columns=transformed_columns)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans=Encoder(df, target, threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df_test as pickle\n",
    "df_test = pd.read_pickle(PATH.PROCESSED + 'df_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=Encoder(df_test, target, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save X_test as pickle\n",
    "X_test.to_pickle(PATH.PROCESSED + 'X_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# División train y test\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la división aprovecho a quitar los outliiers para trabajos los modelos baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.ecod import ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_split(X_trans,y, test_size=0.2):\n",
    "    Xout=X_trans.copy()\n",
    "    outclf = ECOD(n_jobs=-1)\n",
    "    outclf.fit(Xout)\n",
    "    mask = outclf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "    X_trans_out=X_trans[mask==0]\n",
    "    X_trans_out=pd.DataFrame(data=X_trans_out, columns=X_trans.columns)\n",
    "    y_out=[y for  i,y in enumerate(y) if mask[i]==0]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trans_out, y_out, test_size=test_size, random_state=123)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val=outlier_split(X_trans,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval=['X_train', 'X_val','y_train', 'y_val']\n",
    "for data in trainval:\n",
    "    if  isinstance(data,pd.DataFrame):\n",
    "        globals()[data]=reduce_memory_usage(globals()[data])\n",
    "    else:\n",
    "        globals()[data]=reduce_memory_usage(pd.DataFrame((globals()[data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar X_train, X_val, y_train, y_val\n",
    "X_train.to_pickle(PATH.PROCESSED + 'X_train.pkl')\n",
    "X_val.to_pickle(PATH.PROCESSED + 'X_val.pkl')\n",
    "y_train.to_pickle(PATH.PROCESSED + 'y_train.pkl')\n",
    "y_val.to_pickle(PATH.PROCESSED + 'y_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer X_train, X_val, y_train, y_val\n",
    "X_train = pd.read_pickle(PATH.PROCESSED + 'X_train.pkl')\n",
    "X_val = pd.read_pickle(PATH.PROCESSED + 'X_val.pkl')\n",
    "y_train = pd.read_pickle(PATH.PROCESSED + 'y_train.pkl')\n",
    "y_val = pd.read_pickle(PATH.PROCESSED + 'y_val.pkl')\n",
    "X_test = pd.read_pickle(PATH.PROCESSED + 'X_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selvars=['Age',\n",
    " 'CAEC',\n",
    " 'CALC',\n",
    " 'CH2O',\n",
    " 'FAF',\n",
    " 'FCVC',\n",
    " 'Gender_Male',\n",
    " 'HA',\n",
    " 'Height',\n",
    " 'IMC',\n",
    " 'MTRANS_Public_Transportation',\n",
    " 'NCP',\n",
    " 'TUE',\n",
    " 'UT',\n",
    " 'Weight',\n",
    " 'family_history_with_overweight_yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos de Clasificación\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduzco el modelo a las variables seleccionadas\n",
    "X_train_fs=X_train[selvars]\n",
    "X_val_fs=X_val[selvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning por RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models4tune=modelos = {\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'LightGBM':LGBMClassifier(),\n",
    "    'CatBoost': CatBoostClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metamodelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tuned_models from pickle in PATH.MODELS:\n",
    "import pickle\n",
    "with open(PATH.MODELS+'tuned_models.pkl', 'rb') as f:\n",
    "    tuned_models_pkl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models=[\n",
    "    ('XGBoost',tuned_models_pkl['XGBoost']),\n",
    "    ('Gradient Boosting',tuned_models_pkl['Gradient Boosting']),\n",
    "    ('LightGBM',tuned_models_pkl['LightGBM']),\n",
    "    ('CatBoost',tuned_models_pkl['CatBoost'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_fs=X_val[selvars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uno los dataframe X_train_fs y X_val_fs\n",
    "X_train_meta=pd.concat([X_train_fs,X_val_fs],axis=0)\n",
    "y_train_meta=pd.concat([y_train,y_val],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model=tuned_models_pkl['Gradient Boosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "stacking_model.fit(X_train_meta, y_train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stacking model to pickle in PATH.MODELS:\n",
    "import pickle\n",
    "with open(PATH.MODELS+'stacking_model.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
