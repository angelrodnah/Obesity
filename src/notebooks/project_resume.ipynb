{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del Riesgo de Obesidad y problemas CardioVasculares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de librerías y configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga funciones propias reutilizables  de librerías\n",
    "import os as so\n",
    "import sys\n",
    "utils_path = so.path.join(so.getcwd(), '..', 'utils')\n",
    "sys.path.append(utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PATH():\n",
    "    RAW = '../data/raw//'\n",
    "    PROCESSED= '../data/processed//'\n",
    "    MODELS = '../models//'\n",
    "    REPORTS = '../reports//'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas config\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn config\n",
    "colors_palette=sns.color_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BorutaShap import BorutaShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(PATH.RAW + 'train.csv')\n",
    "df_test = pd.read_csv(PATH.RAW +'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significado de las columnas\n",
    "Los datos consisten en la estimación de los niveles de obesidad en personas de los países de México, Perú y Colombia, con edades entre 14 y 61 años y diversos hábitos alimenticios y condiciones físicas. Los datos se recopilaron utilizando una plataforma web con una encuesta donde usuarios anónimos respondieron cada pregunta, luego la información fue procesada obteniendo 17 atributos y 2111 registros.\n",
    "\n",
    "Los atributos relacionados con los hábitos alimenticios son: \n",
    "- Consumo frecuente de alimentos altos en calorías (FAVC)\n",
    "- Frecuencia de consumo de vegetales (FCVC)\n",
    "- Número de comidas principales (NCP)\n",
    "- Consumo de alimentos entre comidas (CAEC)\n",
    "- Consumo de agua diario (CH20)\n",
    "- Consumo de alcohol (CALC)\n",
    "\n",
    "Los atributos relacionados con la condición física son: \n",
    "- Monitoreo del consumo de calorías (SCC)\n",
    "- Frecuencia de actividad física (FAF)\n",
    "- Tiempo utilizando dispositivos tecnológicos (TUE)\n",
    "- Transporte utilizado (MTRANS)\n",
    "\n",
    "Variables obtenidas: Género, Edad, Altura y Peso.\n",
    "\n",
    "Los valores de NObesidad son:\n",
    "\n",
    "- Bajo peso: Menos de 18.5\n",
    "- Normal: 18.5 a 24.9\n",
    "- Sobrepeso: 25.0 a 29.9\n",
    "- Obesidad I: 30.0 a 34.9\n",
    "- Obesidad II: 35.0 a 39.9\n",
    "- Obesidad III: Más de 40\n",
    "\n",
    "Los datos contienen datos numéricos y datos continuos, por lo que pueden ser utilizados para análisis basados en algoritmos de clasificación, predicción, segmentación y asociación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_columnas = {\n",
    "    'FAVC': 'Consumo frecuente de alimentos altos en calorías',\n",
    "    'FCVC': 'Frecuencia de consumo de vegetales',\n",
    "    'NCP': 'Número de comidas principales',\n",
    "    'CAEC': 'Consumo de alimentos entre comidas',\n",
    "    'CH20': 'Consumo de agua diario',\n",
    "    'CALC': 'Consumo de alcohol',\n",
    "    'SCC': 'Monitoreo del consumo de calorías',\n",
    "    'FAF': 'Frecuencia de actividad física',\n",
    "    'TUE': 'Tiempo utilizando dispositivos tecnológicos',\n",
    "    'MTRANS': 'Transporte utilizado',\n",
    "    'Age':'Edad',\n",
    "    'Height': 'Altura',\n",
    "    'Weight':'Peso',\n",
    "    'SMOKE':'Fumador',\n",
    "    'Gender':'Género',\n",
    "    'IMC':'Indice de Masa Corporal',\n",
    "    'HA': 'Hábitos alimienticios',\n",
    "    'UT': 'Uso de Tecnología',\n",
    "    'family_history_with_overweight':'Historial familiar con sobrepeso'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"NObeyesdad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_target={'Insufficient_Weight': 0,\n",
    " 'Normal_Weight': 1,\n",
    " 'Overweight_Level_I': 2,\n",
    " 'Overweight_Level_II': 3,\n",
    "  'Obesity_Type_I': 4,\n",
    " 'Obesity_Type_II': 5,\n",
    " 'Obesity_Type_III': 6,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictrev_target={v:k for k,v in  dict_target.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Vistazo rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlights: Sin missings, no tiene mucha cardinalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_inf = df.isin([np.inf, -np.inf]).any().any()\n",
    "contains_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay infinitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicados = df.duplicated().any()\n",
    "duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = df.columns[df.dtypes==\"object\"].tolist()\n",
    "numericas = df.columns[df.dtypes!=\"object\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tidy_categorical(df, categoricas, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tidy(df, numericas, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema Machine Learning\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target].value_counts() / len(df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al examinar la distribución del objetivo, podemos deducir lo siguiente:\n",
    "\n",
    "- Se trata de un problema de clasificación múltiple, con 7 clases.\n",
    "- Las clases están distribuidas de manera diferente, pero no hay diferencias extremas en sus proporciones (probabilidad media).\n",
    "- Sin embargo, la clase más frecuente (Obesidad_Tipo_III) tiene casi el doble de probabilidad que la menos frecuente (Sobrepeso_Nivel_I). Por lo tanto, al definir la estrategia de validación cruzada, será estratificada  para que las diferentes probabilidades previas se reflejen lo más exactamente posible también en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.stylecraze.com/wp-content/themes/buddyboss-child/images/man-body-mass-index-vector.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Índice de Masa Corporal (IMC):\n",
    "Utilizando las características de 'Altura' y 'Peso'. El IMC, una métrica ampliamente reconocida, indica la obesidad al proporcionar una representación más precisa de la relación entre el peso y la altura de un individuo.\n",
    "\n",
    "Hábitos Alimenticios (HA):\n",
    "La combinación de 'FCVC' (Frecuencia de consumo de vegetales) y 'NCP' (Número de comidas principales) creó la característica 'Hábitos_Alimenticios'. Esta característica busca encapsular los patrones dietéticos generales, considerando tanto la frecuencia de consumo de vegetales como el número de comidas principales.\n",
    "\n",
    "Puntuación de Uso de Tecnología (UT):\n",
    "Se creó una puntuación integral ponderando la frecuencia de uso de la tecnología ('UT') por la edad del individuo. La puntuación resultante de 'Uso_de_Tecnología' tiene como objetivo cuantificar el tiempo promedio que se pasa utilizando la tecnología en relación con la edad de la persona, proporcionando una perspectiva matizada sobre los hábitos tecnológicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay columnas que tienen orden y que deben convertirse a variables discretas. Las mapeo también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_CAEC={\n",
    "    'no': 0,\n",
    "    'Sometimes': 1,\n",
    "    'Frequently': 2,\n",
    "    'Always': 3}\n",
    "dict_CALC={\n",
    "    'no': 0,\n",
    "    'Sometimes': 1,\n",
    "    'Frequently': 2,\n",
    "    'Always': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering(df):\n",
    "    df.set_index('id', inplace=True)\n",
    "    df['IMC'] = df['Weight'] / (df['Height'] ** 2)\n",
    "    df['HA'] = df['FCVC'] * df['NCP']\n",
    "    df['UT'] = df['TUE'] / df['Age']\n",
    "    df['CALC']=df['CALC'].map(dict_CALC)\n",
    "    df['CAEC']=df['CAEC'].map(dict_CAEC)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Feature_Engineering(df_train)\n",
    "df_test = Feature_Engineering(df_test)\n",
    "df = Feature_Engineering(df)\n",
    "numericas = df.columns[df.dtypes!=\"object\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_columnas = {\n",
    "    'FAVC': 'Consumo frecuente de alimentos altos en calorías',\n",
    "    'FCVC': 'Frecuencia de consumo de vegetales',\n",
    "    'NCP': 'Número de comidas principales',\n",
    "    'CAEC': 'Consumo de alimentos entre comidas',\n",
    "    'CH20': 'Consumo de agua diario',\n",
    "    'CALC': 'Consumo de alcohol',\n",
    "    'SCC': 'Monitoreo del consumo de calorías',\n",
    "    'FAF': 'Frecuencia de actividad física',\n",
    "    'TUE': 'Tiempo utilizando dispositivos tecnológicos',\n",
    "    'MTRANS': 'Transporte utilizado',\n",
    "    'Age':'Edad',\n",
    "    'Height': 'Altura',\n",
    "    'Weight':'Peso',\n",
    "    'SMOKE':'Fumador',\n",
    "    'Gender':'Género',\n",
    "    'IMC':'Indice de Masa Corporal',\n",
    "    'HA': 'Hábitos alimienticios',\n",
    "    'UT': 'Uso de Tecnología',\n",
    "    'family_history_with_overweight':'Historial familiar con sobrepeso'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis univariante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas = df.columns[df.dtypes==\"object\"].tolist()\n",
    "numericas = df.columns[df.dtypes!=\"object\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_catplot(df, categoricas, diccionario_columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target]=df[target].map(dict_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numericas:\n",
    "    plot_distribucion(df,col, title=diccionario_columnas.get(col, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis bivariante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='white')\n",
    "sns.pairplot(df,kind=\"reg\",diag_kind='kde',plot_kws={'line_kws':{'color':'red'}},corner=True,hue=target)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numericas:\n",
    "    plot_analysis(df, target, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminación de features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_cols(df, max_cardi=20, max_miss=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phik\n",
    "phik_matrix = df.phik_matrix()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(phik_matrix,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            center=0,\n",
    "            cmap=sns.diverging_palette(145, 280, s=85, l=25, n=10),\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalías y errores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se han encontrado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones (Encodeing)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_transform(X):\n",
    "    return np.sqrt(X)\n",
    "\n",
    "def log_transform(X):\n",
    "    return np.log1p(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_distributions(df, threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_distributions(df.drop(columns=[target]), threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def Encoder(df, target_col, pca_n_components=8, threshold=0.05):\n",
    "    dist_class = classify_distributions(df.drop(columns=[target_col]), threshold)\n",
    "    categorical_columns = list(df.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    if categorical_columns:\n",
    "        categorical_pipeline = Pipeline([\n",
    "            ('onehot', OneHotEncoder(drop='first'))\n",
    "        ])\n",
    "    else:\n",
    "        categorical_pipeline = None\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('transformation', ColumnTransformer([\n",
    "            ('sqrt_transform', FunctionTransformer(sqrt_transform), [col for col, (dist, _) in dist_class.items() if dist == 'positive_increasing']),\n",
    "            ('log_transform', FunctionTransformer(log_transform), [col for col, (dist, _) in dist_class.items() if dist == 'positive_decreasing']),\n",
    "            ('yeojohnson_transform', PowerTransformer(method='yeo-johnson'), [col for col, (dist, _) in dist_class.items() if dist not in ['positive_increasing', 'positive_decreasing']])\n",
    "        ], remainder='passthrough')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    if categorical_pipeline:\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('categorical', categorical_pipeline, categorical_columns),\n",
    "            ('numeric', numeric_pipeline, [col for col, _ in dist_class.items()])\n",
    "        ])\n",
    "    else:\n",
    "        preprocessor = numeric_pipeline\n",
    "\n",
    "    final_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "\n",
    "    required_columns = set([col for col, _ in dist_class.items()])\n",
    "    if not required_columns.issubset(X.columns):\n",
    "        missing_columns = required_columns - set(X.columns)\n",
    "        raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "  \n",
    "    return final_pipeline.fit_transform(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans=Encoder(df, target_col, pca_n_components=8, threshold=0.05)\n",
    "X_test=Encoder(df_test, target_col, pca_n_components=8, threshold=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. División train y test\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Outliners\n",
    "def outlier_split(X_trans,y, test_size=0.2):\n",
    "    \n",
    "    Xout=X_trans.values\n",
    "    tasks = python_clustering.Tasks()\n",
    "    mode = \"overall\"\n",
    "    result, detected_outliers, classifiers = tasks.detect_anomalies(\n",
    "        Xout, methods=\"all_besides_nn\", outliers_fraction=0.05, mode=mode\n",
    "    )\n",
    "    tasks.plot_overall_anomaly_classifiers(result, classifiers, show_heatmap=False)\n",
    "    arr=result[:,-1]\n",
    "    # Calculate the 95th percentile\n",
    "    percentile = np.percentile(arr, 95)\n",
    "\n",
    "    # Create boolean array where values are in the 95th percentile\n",
    "    boolean_array = arr <= percentile\n",
    "    X_trans_out=X_trans[boolean_array]\n",
    "    X_trans_out=pd.DataFrame(data=X_trans_out, columns=X_trans.columns)\n",
    "    y_out=[y for  i,y in enumerate(y) if boolean_array[i]]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trans_out, y_out, test_size=test_size, random_state=123)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val=outlier_split(X_trans,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Feature Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selvars=selvars_boruta(df,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Escoger métrica del modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.1 Métricas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos de Clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir los modelos a comparar\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(solver='liblinear'),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'Support Vector Machine': SVC(gamma='scale', probability=True),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Bagging': BaggingClassifier(n_estimators=10),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=10),\n",
    "    #'XGBoost': XGBClassifier(),\n",
    "    'Random Forest':RandomForestClassifier(n_estimators=10),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'LightGBM':LGBMClassifier(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_df = [\n",
    "    \"X_train\",\n",
    "    \"X_val\",\n",
    "    \"Xtest_set\",\n",
    "    \"y_train\",\n",
    "    \"y_val\",\n",
    "    \"ytest_set\",\n",
    "]\n",
    "for ddf in splits_df:\n",
    "    globals()[ddf] = reduce_memory_usage(pd.DataFrame(globals()[ddf] ), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factores que influyen en esta decisión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df=perform_cross_validation(modelos, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elegir hiperparámetros: Fine tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Según el volumen de datos y sus tipos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = hyperparameter_tuning(models_df, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=results_df.loc[results_df['Best Score']==max(results_df['Best Score']),'Best Parameters'].to_dict()[0]\n",
    "best_model_name=results_df.loc[results_df['Best Score']==max(results_df['Best Score']),'Model'].to_dict()[0]\n",
    "best_model=modelos.get(best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned=best_model.set_params(**best_params)\n",
    "model_tuned.fit(X_train,y_train)\n",
    "y_pred=model_tuned.predict(X_val)\n",
    "recall_score(y_val,y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependerá de cada modelo. Ejecutamos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_with_confusion_matrix(model_tuned, X_val, y_val, nsplits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prestaciones del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification_report = pd.DataFrame.from_dict(classification_report(y_val, y_pred, output_dict=True)).T\n",
    "Classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve y Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama a la función con tu modelo y datos de validación\n",
    "plot_roc_curve(model_tuned, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_roc_auc(model_tuned, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_size_coef=0.2\n",
    "cv=5\n",
    "train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "scoring='recall_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_size=0.15\n",
    "perc_pca=0.95\n",
    "thresh_norm=0.05\n",
    "steps=5\n",
    "score='recall_macro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sizes, train_scores, test_scores=learning_curve(model_tuned, X_train, y_train, cv=ShuffleSplit(n_splits=50, test_size=t_size, random_state=0), n_jobs=-1, train_sizes=np.linspace(0.1, 1.0,steps), scoring=score)\n",
    "display = LearningCurveDisplay(train_sizes=train_sizes,\n",
    "    train_scores=train_scores, test_scores=test_scores, score_name=score)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Bonus Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
